//! Optimization service for intelligent prompt generation and routing strategy tuning
//!
//! This service analyzes producer performance statistics and generates optimized
//! prompts and routing strategies to maximize unique attributes per minute.

use std::collections::HashMap;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use async_trait::async_trait;

use shared::{ProviderId, RoutingStrategy, GenerationConfig, LLMPerformance};
use crate::error::OrchestratorResult;
use crate::traits;

/// Statistics aggregated from all producers for optimization decisions
#[derive(Debug, Clone)]
pub struct OptimizationStats {
    /// Total unique attributes generated per minute across all producers
    pub unique_attrs_per_minute: f64,
    /// Performance data per LLM provider across all producers
    pub provider_performance: HashMap<ProviderId, AggregatedProviderStats>,
    /// Current topic being processed
    pub current_topic: Option<String>,
    /// Number of active producers
    pub active_producers: u32,
    /// System uptime in seconds
    pub uptime_seconds: u64,
}

/// Aggregated performance statistics for a specific provider across all producers
#[derive(Debug, Clone)]
pub struct AggregatedProviderStats {
    /// Total successful requests across all producers
    pub total_successful: u64,
    /// Total failed requests across all producers  
    pub total_failed: u64,
    /// Average response time in milliseconds
    pub avg_response_time_ms: f64,
    /// Average success rate (0.0 to 1.0)
    pub avg_success_rate: f64,
    /// Average uniqueness ratio of generated attributes
    pub avg_uniqueness_ratio: f64,
    /// Current provider health status
    pub current_status: shared::ProviderStatus,
}

/// Optimization recommendation generated by the optimizer
#[derive(Debug, Clone)]
pub struct OptimizationPlan {
    /// Optimized prompt for the given topic
    pub optimized_prompt: String,
    /// Recommended routing strategy based on performance
    pub routing_strategy: RoutingStrategy,
    /// Generation configuration adjustments
    pub generation_config: GenerationConfig,
    /// Whether this plan requires sending producer commands
    pub requires_update: bool,
    /// Confidence score for this optimization (0.0 to 1.0)
    pub confidence: f64,
    /// Explanation of why this optimization was chosen
    pub rationale: String,
}

/// Partitioning strategy for distributing work across producers
#[derive(Debug, Clone)]
pub enum PartitioningStrategy {
    /// No partitioning - all producers use same prompt
    None,
    /// Semantic partitioning - divide topic into semantic categories
    Semantic { categories: Vec<String> },
    /// Attribute type partitioning - focus different producers on different attribute types
    AttributeType { types: Vec<String> },
    /// Provider-specific partitioning - leverage different provider strengths
    ProviderSpecific { specializations: HashMap<ProviderId, String> },
}

/// Optimizer trait for generating intelligent prompts and routing strategies
#[mockall::automock]
#[async_trait]
pub trait Optimizer: Send + Sync {
    /// Generate optimized prompt and routing strategy for a given topic
    /// 
    /// # Parameters
    /// - `topic`: Raw topic from web UI
    /// - `current_stats`: Current system performance statistics
    /// 
    /// # Returns
    /// Optimization plan with prompt, routing strategy, and confidence metrics
    async fn optimize_for_topic(&self, topic: &str, current_stats: &OptimizationStats) -> OrchestratorResult<OptimizationPlan>;
    
    /// Analyze current performance and suggest optimizations
    /// 
    /// # Parameters
    /// - `current_stats`: Performance statistics from all producers
    /// 
    /// # Returns
    /// Optional optimization plan if improvements are recommended
    async fn analyze_and_suggest(&self, current_stats: &OptimizationStats) -> OrchestratorResult<Option<OptimizationPlan>>;
    
    /// Generate partitioning strategy based on topic and provider performance
    /// 
    /// # Parameters
    /// - `topic`: Topic to be partitioned
    /// - `provider_stats`: Performance data for each provider
    /// 
    /// # Returns
    /// Recommended partitioning strategy
    async fn generate_partitioning_strategy(&self, topic: &str, provider_stats: &HashMap<ProviderId, AggregatedProviderStats>) -> OrchestratorResult<PartitioningStrategy>;
}

/// Real optimizer implementation using heuristics and performance analysis
pub struct RealOptimizer {
    /// Minimum confidence threshold for applying optimizations
    confidence_threshold: f64,
    /// Historical performance tracking for trend analysis
    performance_history: std::sync::Arc<tokio::sync::RwLock<Vec<OptimizationStats>>>,
}

impl RealOptimizer {
    /// Create new optimizer with default configuration
    pub fn new() -> Self {
        Self {
            confidence_threshold: 0.7, // Only apply optimizations with >70% confidence
            performance_history: std::sync::Arc::new(tokio::sync::RwLock::new(Vec::new())),
        }
    }
    
    /// Create optimizer with custom confidence threshold
    pub fn with_confidence_threshold(threshold: f64) -> Self {
        Self {
            confidence_threshold: threshold,
            performance_history: std::sync::Arc::new(tokio::sync::RwLock::new(Vec::new())),
        }
    }
    
    /// Generate base prompt for a topic with optional partitioning
    fn generate_base_prompt(&self, topic: &str, partitioning: &PartitioningStrategy) -> String {
        match partitioning {
            PartitioningStrategy::None => {
                format!(
                    "Generate diverse, creative, and unique attributes related to '{}'. \
                    Focus on originality and avoid common or obvious attributes. \
                    Provide specific, actionable, and measurable attributes.",
                    topic
                )
            },
            PartitioningStrategy::Semantic { categories } => {
                format!(
                    "Generate unique attributes for '{}' focusing specifically on these aspects: {}. \
                    Be creative and specific within your assigned focus areas. \
                    Avoid generic attributes and prioritize uniqueness.",
                    topic,
                    categories.join(", ")
                )
            },
            PartitioningStrategy::AttributeType { types } => {
                format!(
                    "Generate unique attributes for '{}' specifically of these types: {}. \
                    Focus on your assigned attribute types and ensure high uniqueness. \
                    Be detailed and specific within your type specialization.",
                    topic,
                    types.join(", ")
                )
            },
            PartitioningStrategy::ProviderSpecific { specializations: _ } => {
                // For provider-specific, we'll customize per provider later
                format!(
                    "Generate unique attributes for '{}' leveraging your specialized capabilities. \
                    Focus on areas where you excel and ensure maximum uniqueness and creativity.",
                    topic
                )
            },
        }
    }
    
    /// Analyze provider performance to determine optimal routing strategy
    fn analyze_routing_performance(&self, provider_stats: &HashMap<ProviderId, AggregatedProviderStats>) -> RoutingStrategy {
        if provider_stats.is_empty() {
            // Default fallback
            return RoutingStrategy::RoundRobin {
                providers: vec![ProviderId::OpenAI, ProviderId::Anthropic, ProviderId::Gemini]
            };
        }
        
        // Calculate performance scores for each provider
        let mut provider_scores: Vec<(ProviderId, f64)> = provider_stats
            .iter()
            .map(|(provider_id, stats)| {
                // Composite score considering success rate, speed, and uniqueness
                let success_weight = 0.4;
                let speed_weight = 0.3; 
                let uniqueness_weight = 0.3;
                
                let speed_score = if stats.avg_response_time_ms > 0.0 {
                    // Lower response time = higher score (capped at reasonable limits)
                    1.0 - (stats.avg_response_time_ms.min(5000.0) / 5000.0)
                } else {
                    0.5 // Unknown performance
                };
                
                let score = (stats.avg_success_rate * success_weight) + 
                           (speed_score * speed_weight) + 
                           (stats.avg_uniqueness_ratio * uniqueness_weight);
                           
                (provider_id.clone(), score)
            })
            .collect();
            
        // Sort by performance score (highest first)
        provider_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        
        // Determine strategy based on performance distribution
        let top_score = provider_scores.first().map(|(_, score)| *score).unwrap_or(0.0);
        let score_variance = self.calculate_score_variance(&provider_scores);
        
        if score_variance < 0.1 {
            // All providers perform similarly - use round robin
            let providers: Vec<ProviderId> = provider_scores.into_iter().map(|(id, _)| id).collect();
            RoutingStrategy::RoundRobin { providers }
        } else if top_score > 0.8 {
            // Clear winner - use priority order with best first
            let providers: Vec<ProviderId> = provider_scores.into_iter().map(|(id, _)| id).collect();
            RoutingStrategy::PriorityOrder { providers }
        } else {
            // Mixed performance - use weighted strategy
            let mut weights = HashMap::new();
            let total_score: f64 = provider_scores.iter().map(|(_, score)| score).sum();
            
            for (provider_id, score) in provider_scores {
                let weight = if total_score > 0.0 { score / total_score } else { 1.0 };
                weights.insert(provider_id, weight as f32);
            }
            
            RoutingStrategy::Weighted { weights }
        }
    }
    
    /// Calculate variance in performance scores
    fn calculate_score_variance(&self, scores: &[(ProviderId, f64)]) -> f64 {
        if scores.len() < 2 {
            return 0.0;
        }
        
        let mean: f64 = scores.iter().map(|(_, score)| score).sum::<f64>() / scores.len() as f64;
        let variance: f64 = scores
            .iter()
            .map(|(_, score)| (score - mean).powi(2))
            .sum::<f64>() / scores.len() as f64;
            
        variance
    }
    
    /// Calculate optimization confidence based on data quality and historical trends
    fn calculate_confidence(&self, current_stats: &OptimizationStats) -> f64 {
        let mut confidence: f64 = 0.5; // Base confidence
        
        // Increase confidence with more data
        if current_stats.active_producers >= 3 {
            confidence += 0.2;
        }
        
        // Increase confidence with longer runtime (more stable metrics)
        if current_stats.uptime_seconds > 300 { // 5 minutes
            confidence += 0.1;
        }
        
        // Increase confidence if we have provider performance data
        if !current_stats.provider_performance.is_empty() {
            confidence += 0.1;
        }
        
        // Increase confidence if providers have sufficient request volume
        let has_sufficient_data = current_stats.provider_performance
            .values()
            .any(|stats| stats.total_successful + stats.total_failed >= 10);
            
        if has_sufficient_data {
            confidence += 0.1;
        }
        
        confidence.min(1.0)
    }
}

impl RealOptimizer {
    /// Calculate variance in provider performance to determine if specialization would help  
    fn calculate_provider_performance_variance(&self, provider_stats: &HashMap<ProviderId, AggregatedProviderStats>) -> f64 {
        if provider_stats.len() < 2 {
            return 0.0;
        }
        
        let scores: Vec<f64> = provider_stats
            .values()
            .map(|stats| stats.avg_success_rate * stats.avg_uniqueness_ratio)
            .collect();
            
        let mean: f64 = scores.iter().sum::<f64>() / scores.len() as f64;
        let variance: f64 = scores
            .iter()
            .map(|score| (score - mean).powi(2))
            .sum::<f64>() / scores.len() as f64;
            
        variance
    }
    
    /// Extract semantic categories from a topic for partitioning
    fn extract_semantic_categories(&self, topic: &str) -> Vec<String> {
        let words: Vec<&str> = topic.split_whitespace().collect();
        
        // Simple heuristic - split into conceptual categories
        // In a more sophisticated implementation, this could use NLP
        match words.len() {
            1..=2 => vec![format!("core {}", topic), format!("related {}", topic), format!("extended {}", topic)],
            3..=4 => {
                let mid = words.len() / 2;
                vec![
                    words[..mid].join(" "),
                    words[mid..].join(" "),
                    format!("combined {}", topic)
                ]
            },
            _ => {
                // Complex topic - create semantic groupings
                vec![
                    format!("primary aspects of {}", topic),
                    format!("secondary features of {}", topic), 
                    format!("contextual elements of {}", topic)
                ]
            }
        }
    }
}

#[async_trait]
impl Optimizer for RealOptimizer {
    async fn optimize_for_topic(&self, topic: &str, current_stats: &OptimizationStats) -> OrchestratorResult<OptimizationPlan> {
        // Store current stats in history
        {
            let mut history = self.performance_history.write().await;
            history.push(current_stats.clone());
            
            // Keep only recent history (last 100 entries)
            if history.len() > 100 {
                let len = history.len();
                history.drain(0..len-100);
            }
        }
        
        // Generate partitioning strategy
        let partitioning = self.generate_partitioning_strategy(topic, &current_stats.provider_performance).await?;
        
        // Generate optimized prompt
        let optimized_prompt = self.generate_base_prompt(topic, &partitioning);
        
        // Determine optimal routing strategy
        let routing_strategy = self.analyze_routing_performance(&current_stats.provider_performance);
        
        // Generate appropriate generation config
        let generation_config = GenerationConfig {
            model: "default".to_string(), // Will be overridden per provider
            batch_size: 1,
            context_window: 4096,
            max_tokens: 800, // Balanced for attribute generation
            temperature: 0.8, // Higher creativity for uniqueness
        };
        
        // Calculate confidence
        let confidence = self.calculate_confidence(current_stats);
        
        // Determine if update is needed
        let requires_update = current_stats.current_topic.as_ref() != Some(&topic.to_string());
        
        let rationale = format!(
            "Topic optimization for '{}': Generated {} prompt with {} routing strategy. \
            Confidence: {:.1}% based on {} active producers and {} provider performance data.",
            topic,
            match partitioning {
                PartitioningStrategy::None => "unified",
                PartitioningStrategy::Semantic { .. } => "semantically partitioned",
                PartitioningStrategy::AttributeType { .. } => "attribute-type partitioned", 
                PartitioningStrategy::ProviderSpecific { .. } => "provider-specialized",
            },
            match routing_strategy {
                RoutingStrategy::RoundRobin { .. } => "round-robin",
                RoutingStrategy::PriorityOrder { .. } => "priority-based",
                RoutingStrategy::Weighted { .. } => "weighted",
            },
            confidence * 100.0,
            current_stats.active_producers,
            current_stats.provider_performance.len()
        );
        
        Ok(OptimizationPlan {
            optimized_prompt,
            routing_strategy,
            generation_config,
            requires_update,
            confidence,
            rationale,
        })
    }
    
    async fn analyze_and_suggest(&self, current_stats: &OptimizationStats) -> OrchestratorResult<Option<OptimizationPlan>> {
        // Only suggest optimizations if we have sufficient data
        if current_stats.active_producers == 0 || current_stats.uptime_seconds < 60 {
            return Ok(None);
        }
        
        // Check if current performance is suboptimal
        let needs_optimization = current_stats.unique_attrs_per_minute < 5.0 || // Low generation rate
            current_stats.provider_performance.values().any(|stats| stats.avg_success_rate < 0.8); // High failure rate
        
        if !needs_optimization {
            return Ok(None);
        }
        
        // If we have a current topic, optimize for it
        if let Some(topic) = &current_stats.current_topic {
            let plan = self.optimize_for_topic(topic, current_stats).await?;
            
            // Only return plan if confidence exceeds threshold
            if plan.confidence >= self.confidence_threshold {
                Ok(Some(plan))
            } else {
                Ok(None)
            }
        } else {
            Ok(None)
        }
    }
    
    async fn generate_partitioning_strategy(&self, topic: &str, provider_stats: &HashMap<ProviderId, AggregatedProviderStats>) -> OrchestratorResult<PartitioningStrategy> {
        // For now, use simple heuristics - could be enhanced with ML in the future
        
        // If we don't have provider performance data, don't partition
        if provider_stats.is_empty() {
            return Ok(PartitioningStrategy::None);
        }
        
        // Analyze topic complexity
        let word_count = topic.split_whitespace().count();
        let has_multiple_concepts = topic.contains(" and ") || topic.contains(",") || word_count > 3;
        
        if has_multiple_concepts && provider_stats.len() >= 3 {
            // Topic is complex enough and we have enough providers for partitioning
            
            // Check if providers have different performance characteristics
            let performance_variance = self.calculate_provider_performance_variance(provider_stats);
            
            if performance_variance > 0.1 {
                // Providers perform differently - use provider-specific specialization
                let mut specializations = HashMap::new();
                
                // Assign specializations based on provider strengths
                for (provider_id, _stats) in provider_stats {
                    let specialization = match provider_id {
                        ProviderId::OpenAI => "technical and analytical attributes",
                        ProviderId::Anthropic => "creative and conceptual attributes", 
                        ProviderId::Gemini => "practical and functional attributes",
                    };
                    specializations.insert(provider_id.clone(), specialization.to_string());
                }
                
                Ok(PartitioningStrategy::ProviderSpecific { specializations })
            } else {
                // Providers perform similarly - use semantic partitioning
                let categories = self.extract_semantic_categories(topic);
                Ok(PartitioningStrategy::Semantic { categories })
            }
        } else {
            // Topic is simple or we don't have enough providers - no partitioning
            Ok(PartitioningStrategy::None)
        }
    }
}

/// Implement the traits::Optimizer interface for RealOptimizer
#[async_trait]
impl traits::Optimizer for RealOptimizer {
    async fn optimize_for_topic(&self, topic: &str, current_stats: &OptimizationStats) -> OrchestratorResult<OptimizationPlan> {
        // Direct call to the internal Optimizer trait implementation
        Optimizer::optimize_for_topic(self, topic, current_stats).await
    }
    
    async fn analyze_and_suggest(&self, current_stats: &OptimizationStats) -> OrchestratorResult<Option<OptimizationPlan>> {
        // Direct call to the internal Optimizer trait implementation
        Optimizer::analyze_and_suggest(self, current_stats).await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_optimizer_creation() {
        let optimizer = RealOptimizer::new();
        assert!(true); // Basic creation test
    }
    
    #[tokio::test]
    async fn test_topic_optimization() {
        let optimizer = RealOptimizer::new();
        let stats = OptimizationStats {
            unique_attrs_per_minute: 3.5,
            provider_performance: HashMap::new(),
            current_topic: None,
            active_producers: 2,
            uptime_seconds: 300,
        };
        
        let plan = optimizer.optimize_for_topic("sustainable technology", &stats).await;
        assert!(plan.is_ok());
        
        let plan = plan.unwrap();
        assert!(plan.optimized_prompt.contains("sustainable technology"));
        assert!(plan.confidence >= 0.0 && plan.confidence <= 1.0);
    }
}